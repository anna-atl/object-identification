{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is needed for preparing the input datasets: upper case and etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The input datasets should be in the following format:\n",
    "- Company name (string)\n",
    "- Marketplace (string)\n",
    "- Country (string)\n",
    "- State (string)\n",
    "- City (string)\n",
    "- Zip Code (string)\n",
    "- Street (string)\n",
    "- URL (string)\n",
    "- Industry (SIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import sys\n",
    "import numpy as np\n",
    "import string\n",
    "import collections\n",
    "from textblob import TextBlob\n",
    "import Levenshtein\n",
    "import binascii\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_import():\n",
    "    # import FR data (adjustments for delimeters and encoding - latin)\n",
    "    df = pd.read_csv(\"~/Dropbox/Botva/TUM/Master_Thesis/object-identification/datasets/processed_files/france_rna_processed.csv\", encoding='latin-1', sep = ';', error_bad_lines=False) \n",
    "#    df = pd.read_csv(\"~/Dropbox/Botva/TUM/Master_Thesis/object-identification/datasets/raw_files/rna_waldec_20201201_dpt_01.csv\", error_bad_lines=False)\n",
    "#    df = df.astype(str)\n",
    "    print(df)\n",
    "    print(df.dtypes)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_prepare(df):\n",
    "    df = df.apply(lambda x: x.astype(str).str.upper())\n",
    "    df['name'] = df['name'].apply(lambda x: x.replace('.',''))\n",
    "    df['name'] = df['name'].str.replace('[^0-9a-zA-Z]+', ' ')\n",
    "    df['name'] = df['name'].str.replace(' +', ' ')\n",
    "    df['name_split'] = df['name'].str.split(' ')\n",
    "    print(df)\n",
    "    print(df.dtypes)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequent_words(df_processed):\n",
    "    b = TextBlob(\"bonjour\")\n",
    "    b.detect_language()\n",
    "    print(df_processed['name'])\n",
    "    all_words = df_processed['name']    \n",
    "    all_words_cleaned = []\n",
    "\n",
    "    for text in all_words:\n",
    "        text = [x.strip(string.punctuation) for x in text]\n",
    "        all_words_cleaned.append(text)\n",
    "\n",
    "    all_words_cleaned[0]\n",
    "\n",
    "    text_words = [\" \".join(text) for text in all_words_cleaned]\n",
    "    final_text_words = \" \".join(text_words)\n",
    "    #final_text_words[:1000]\n",
    "\n",
    "    print(all_words)\n",
    "    stopwords = set(STOPWORDS)\n",
    "    stopwords.update([\"LE\",\"DE\",\"LA\",\"ET\",\"DES\",\"DU\",\"LES\",\"EN\",\"ET\",\"A\",\"POUR\",\"SUR\",\"SOU\",\"S\",\"D\",\"L\"])\n",
    "\n",
    "    wordcloud_names = WordCloud(stopwords=stopwords, background_color=\"white\", max_font_size=50, max_words=100).generate(final_text_words)\n",
    "\n",
    "    # Lines 4 to 7\n",
    "    plt.figure(figsize = (15,15))\n",
    "    plt.imshow(wordcloud_names, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    filtered_words = [word for word in final_text_words.split() if word not in stopwords]\n",
    "    counted_words = collections.Counter(filtered_words)\n",
    "\n",
    "    word_count = {}\n",
    "\n",
    "    for letter, count in counted_words.most_common(100):\n",
    "        word_count[letter] = count\n",
    "\n",
    "    for i,j in word_count.items():\n",
    "        print('Word: {0}, count: {1}'.format(i,j))\n",
    "    \n",
    "    return word_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Levenstein Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_1 = df_processed['name']\n",
    "texts_2 = df_processed['name']\n",
    "lvn_array = np.zeros((len(texts_1),len(texts_2)))\n",
    "print(lvn_array)\n",
    "\n",
    "text_1_num = 0\n",
    "text_2_num = 0\n",
    "\n",
    "for text_1 in texts_1:\n",
    "    for text_2 in texts_2:\n",
    "        lvn_array[text_1_num,text_2_num] = Levenshtein.ratio(text_1,text_2)\n",
    "        text_2_num += 1 \n",
    "    text_2_num = 0    \n",
    "    text_1_num += 1 \n",
    "lvn_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n = 2\n",
    "shingles_list = []\n",
    "\n",
    "texts = df_processed['name']\n",
    "hashes_list = np.array([0] * len(df_processed['name']))\n",
    "hashes_array = hashes_list\n",
    "\n",
    "text_num = 0\n",
    "shingle_num = 0\n",
    "\n",
    "for text in texts:\n",
    "#    print(text)\n",
    "#    text = text.encode()\n",
    "    shingles = [text[i:i + n] for i in range(len(text) - n + 1)]\n",
    "#    print(shingles)\n",
    "    for shingle in shingles:\n",
    "        if shingle not in shingles_list:\n",
    "            hashes_array = np.vstack((hashes_array, hashes_list)) #needs to be removed after last iteration\n",
    "            shingles_list.append(shingle)\n",
    "            hashes_array[shingle_num,text_num] = 1\n",
    "            shingle_num += 1\n",
    "        if shingle in shingles_list:\n",
    "            hashes_array[shingles_list.index(shingle),text_num] = 1\n",
    "    text_num += 1\n",
    "    \n",
    "hashes_array\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_import()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed = df_prepare(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df_processed['name']\n",
    "\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shingles_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first, set of shingles\n",
    "then, array (check from set)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2\n",
    "shingles_list = set()\n",
    "\n",
    "for text in texts:\n",
    "#    print(text)\n",
    "#    text = text.encode()\n",
    "    shingles = [text[i:i + n] for i in range(len(text) - n + 1)]\n",
    "#    print(shingles)\n",
    "    for shingle in shingles:\n",
    "        if shingle not in shingles_list: #check, maybe if is not needed bc its a set\n",
    "            shingles_list.add(shingle)\n",
    "    \n",
    "shingles_list = sorted(shingles_list)\n",
    "shingles_dict = dict(zip(shingles_list,range(len(shingles_list))))\n",
    "shingles_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [[] for i in range(len(df_processed['name']))]\n",
    "\n",
    "#hashes_array = np.zeros((len(shingles_list), len(df_processed['name'])))\n",
    "\n",
    "for doc, text in zip(docs, texts):\n",
    "    shingles = [text[i:i + n] for i in range(len(text) - n + 1)]    \n",
    "    for shingle in shingles:\n",
    "        doc.append(shingles_dict[shingle])\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(len(d) for d in texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signature_size = 50 #signature size\n",
    "#signatures = [[] for i in range(len(signature_size))]\n",
    "\n",
    "signatures = np.zeros((signature_size, len(docs)))\n",
    "\n",
    "#signatures = [[] for i in range(len(signature_size))]\n",
    "\n",
    "shingles_shuffled = [i for i in range(len(shingles_list))]\n",
    "\n",
    "for signature in signatures:\n",
    "    random.shuffle(shingles_shuffled)    \n",
    "    for doc_index, doc in enumerate(docs):\n",
    "        doc_a = [shingles_shuffled[i] for i in doc]\n",
    "        signature[doc_index] = min(doc_a)\n",
    "\n",
    "signatures\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    for num, name in enumerate(presidents, start=1):\n",
    "    print(\"President {}: {}\".format(num, name))\n",
    "    \n",
    "    for shingle in shingles_shuffled:\n",
    "        while all columns of signature are filled:\n",
    "            signature[doc] = shingle[doc] \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    for i, doc in zip(signature,docs):\n",
    "        shingles_dict_sh\n",
    "        i = doc[shingle]\n",
    "            print(signatures)\n",
    "            break\n",
    "#             = \n",
    "        break\n",
    "    break\n",
    "x =+ 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Levenshtein.ratio(df_processed['name'],df_processed['name'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textdistance import levenshtein\n",
    "\n",
    "df_1 = df_processed['name']\n",
    "df_2 = df_processed['name']\n",
    "df_1.apply(lambda x: levenshtein.distance(df_1['name'], df_2['name']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [{} for i in range(len(df_processed['name']))]\n",
    "\n",
    "#hashes_array = np.zeros((len(shingles_list), len(df_processed['name'])))\n",
    "\n",
    "for doc, text in zip(docs, texts):\n",
    "    shingles = [text[i:i + n] for i in range(len(text) - n + 1)]    \n",
    "    for shingle in shingles:\n",
    "        doc[shingle] = shingles.index(shingle)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install textdistance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
