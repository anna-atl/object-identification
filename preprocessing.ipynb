{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is needed for preparing the input datasets: upper case and etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The input datasets should be in the following format:\n",
    "- Company name (string)\n",
    "- Marketplace (string)\n",
    "- Country (string)\n",
    "- State (string)\n",
    "- City (string)\n",
    "- Zip Code (string)\n",
    "- Street (string)\n",
    "- URL (string)\n",
    "- Industry (SIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import sys\n",
    "import numpy as np\n",
    "import string\n",
    "import collections\n",
    "from textblob import TextBlob\n",
    "import Levenshtein\n",
    "import binascii\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_import():\n",
    "    # import FR data (adjustments for delimeters and encoding - latin)\n",
    "    wordbook_name = \"~/Dropbox/Botva/TUM/Master_Thesis/datasets/processed_files/france_rna_processed.csv\"\n",
    "    df = pd.read_csv(wordbook_name, encoding='latin-1', sep = ';', error_bad_lines=False)\n",
    "#    df = pd.read_csv(\"~/Dropbox/Botva/TUM/Master_Thesis/object-identification/datasets/raw_files/rna_waldec_20201201_dpt_01.csv\", error_bad_lines=False)\n",
    "#    df = df.astype(str)\n",
    "    print(df)\n",
    "    print(df.dtypes)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_prepare(df):\n",
    "    df = df.apply(lambda x: x.astype(str).str.upper())\n",
    "    df['name'] = df['name'].apply(lambda x: x.replace('.',''))\n",
    "    df['name'] = df['name'].str.replace('[^0-9a-zA-Z]+', ' ')\n",
    "    df['name'] = df['name'].str.replace(' +', ' ')\n",
    "    df['name_split'] = df['name'].str.split(' ')\n",
    "    print(df)\n",
    "    print(df.dtypes)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequent_words(df_processed):\n",
    "    b = TextBlob(\"bonjour\")\n",
    "    b.detect_language()\n",
    "    print(df_processed['name'])\n",
    "    all_words = df_processed['name']    \n",
    "    all_words_cleaned = []\n",
    "\n",
    "    for text in all_words:\n",
    "        text = [x.strip(string.punctuation) for x in text]\n",
    "        all_words_cleaned.append(text)\n",
    "\n",
    "    all_words_cleaned[0]\n",
    "\n",
    "    text_words = [\" \".join(text) for text in all_words_cleaned]\n",
    "    final_text_words = \" \".join(text_words)\n",
    "    #final_text_words[:1000]\n",
    "\n",
    "    print(all_words)\n",
    "    stopwords = set(STOPWORDS)\n",
    "    stopwords.update([\"LE\",\"DE\",\"LA\",\"ET\",\"DES\",\"DU\",\"LES\",\"EN\",\"ET\",\"A\",\"POUR\",\"SUR\",\"SOU\",\"S\",\"D\",\"L\"])\n",
    "\n",
    "    wordcloud_names = WordCloud(stopwords=stopwords, background_color=\"white\", max_font_size=50, max_words=100).generate(final_text_words)\n",
    "\n",
    "    # Lines 4 to 7\n",
    "    plt.figure(figsize = (15,15))\n",
    "    plt.imshow(wordcloud_names, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    filtered_words = [word for word in final_text_words.split() if word not in stopwords]\n",
    "    counted_words = collections.Counter(filtered_words)\n",
    "\n",
    "    word_count = {}\n",
    "\n",
    "    for letter, count in counted_words.most_common(100):\n",
    "        word_count[letter] = count\n",
    "\n",
    "    for i,j in word_count.items():\n",
    "        print('Word: {0}, count: {1}'.format(i,j))\n",
    "    \n",
    "    return word_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Levenstein Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenstein_distance(texts_1,texts_2):\n",
    "    lvn_array = np.zeros((len(texts_1),len(texts_2)))\n",
    "    print(lvn_array)\n",
    "\n",
    "    text_1_num = 0\n",
    "    text_2_num = 0\n",
    "\n",
    "    for text_1 in texts_1:\n",
    "        for text_2 in texts_2:\n",
    "            lvn_array[text_1_num,text_2_num] = Levenshtein.ratio(text_1,text_2)\n",
    "            text_2_num += 1 \n",
    "        text_2_num = 0    \n",
    "        text_1_num += 1 \n",
    "    return lvn_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_import()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed = df_prepare(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating shingles_dict\n",
    "def create_shingles_dict(texts,n):\n",
    "    shingles_list = set()\n",
    "\n",
    "    for text in texts:\n",
    "    #    print(text)\n",
    "    #    text = text.encode()\n",
    "        shingles = [text[i:i + n] for i in range(len(text) - n + 1)]\n",
    "    #    print(shingles)\n",
    "        for shingle in shingles:\n",
    "            if shingle not in shingles_list: #check, maybe if is not needed bc its a set\n",
    "                shingles_list.add(shingle)\n",
    "\n",
    "    shingles_list = sorted(shingles_list)\n",
    "    shingles_dict = dict(zip(shingles_list,range(len(shingles_list))))\n",
    "    return shingles_list, shingles_dict\n",
    "\n",
    "texts = df_processed['name']\n",
    "shingles_list, shingles_dict = create_shingles_dict(texts,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting docs to shingles\n",
    "def create_doc_shingles(texts):\n",
    "    docs = [[] for i in range(len(texts))]\n",
    "\n",
    "    for doc, text in zip(docs, texts):\n",
    "        shingles = [text[i:i + n] for i in range(len(text) - n + 1)]    \n",
    "        for shingle in shingles:\n",
    "            doc.append(shingles_dict[shingle])\n",
    "    return docs\n",
    "\n",
    "docs = create_doc_shingles(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating signatures array\n",
    "def create_signatures_array(docs,shingles_list,signature_size):\n",
    "    signatures = np.zeros((signature_size, len(docs))) #create an array\n",
    "\n",
    "    shingles_shuffled = [i for i in range(len(shingles_list))]\n",
    "\n",
    "    for signature in signatures:\n",
    "        random.shuffle(shingles_shuffled)    \n",
    "        for doc_index, doc in enumerate(docs):\n",
    "            doc_a = [shingles_shuffled[i] for i in doc]\n",
    "            signature[doc_index] = min(doc_a)\n",
    "    return signatures\n",
    "\n",
    "signatures = create_signatures_array(docs,shingles_list,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def create_buckets(signatures,bands):\n",
    "    r = int(len(signatures)/bands) #rows per band\n",
    "\n",
    "    buckets_bands = [{} for i in range(bands)]\n",
    "\n",
    "    for band, buckets in zip(range(bands),buckets_bands):\n",
    "        for i in range(len(signatures[0])):\n",
    "            buckets.setdefault(tuple(signatures[band*r:band*r+r,i]),[]).append(i)\n",
    "        filtered = {key:item for key, item in buckets.items() if len(item)>1}\n",
    "        buckets.clear()\n",
    "        buckets.update(filtered)\n",
    "    return buckets_bands\n",
    "\n",
    "buckets_bands = create_buckets(signatures,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(list1, list2):\n",
    "    intersection = len(set(list1).intersection(list2))\n",
    "    union = (len(set(list1)) + len(set(list2))) - intersection\n",
    "    return intersection/union\n",
    "\n",
    "matches = {}\n",
    "\n",
    "for buckets in buckets_bands:\n",
    "    for key, values_list in buckets.items():\n",
    "        for value_1 in values_list:\n",
    "            for value_2 in values_list:\n",
    "                if value_2 > value_1:\n",
    "                    matches.setdefault((value_1,value_2),[]).append(jaccard(docs[value_1], docs[value_2]))\n",
    "                    #print(value_1)\n",
    "                    #print(value_2)\n",
    "#                    print(texts[value_1])\n",
    "#                    print(texts[value_2])\n",
    "#                    print(jaccard(docs[value_1], docs[value_2]))\n",
    "\n",
    "#threshold = 0.7\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(signatures[:,0])\n",
    "print(jaccard(docs[0], docs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def jaccard(list1, list2):\n",
    "#    print(len(list(set(list1).intersection(list2))))\n",
    "    intersection = len(set(list1).intersection(list2))\n",
    "#    print(len(list1))\n",
    "    union = (len(set(list1)) + len(set(list2))) - intersection\n",
    "    return intersection/union\n",
    "\n",
    "print(signatures[:,0])\n",
    "print(jaccard(docs[0], docs[0]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "threshold = 0.5\n",
    "a = len(signatures[0])\n",
    "bucket = {}\n",
    "\n",
    "for band in range(bands):\n",
    "    for i in range(len(signatures[0])):\n",
    "    #    t0 = time.time()\n",
    "        for j in range(len(signatures[0])):\n",
    "            if i<j:\n",
    "                if jaccard(signatures[band*r:band*r+r,i], signatures[band*r:band*r+r,j])>threshold:\n",
    "#                    print(jaccard(signatures[band*r:band*r+r,i], signatures[band*r:band*r+r,j]))\n",
    "#                    print(i,j)\n",
    "                    print(texts[i])\n",
    "                    print(texts[j])\n",
    "                    bucket.setdefault(i,[]).append(j)\n",
    "                    print(bucket)\n",
    "    #    elapsed = (time.time() - t0)            \n",
    "    #    print(\"\\nChecking one doc to all others took %.2fsec\" % elapsed)  \n",
    "    #    b = (a-i)*elapsed/60/60\n",
    "    #    print(\"\\nExpected left time: %.2fhours\" % b)  \n",
    " \n",
    "\n",
    "\n",
    "#compare row by row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(len(d) for d in texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Levenshtein.ratio(df_processed['name'],df_processed['name'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textdistance import levenshtein\n",
    "\n",
    "df_1 = df_processed['name']\n",
    "df_2 = df_processed['name']\n",
    "df_1.apply(lambda x: levenshtein.distance(df_1['name'], df_2['name']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install textdistance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
