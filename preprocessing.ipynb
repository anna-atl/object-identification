{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is needed for preparing the input datasets: upper case and etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The input datasets should be in the following format:\n",
    "- Company name (string)\n",
    "- Marketplace (string)\n",
    "- Country (string)\n",
    "- State (string)\n",
    "- City (string)\n",
    "- Zip Code (string)\n",
    "- Street (string)\n",
    "- URL (string)\n",
    "- Industry (SIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import sys\n",
    "import numpy as np\n",
    "import string\n",
    "import collections\n",
    "from textblob import TextBlob\n",
    "import Levenshtein\n",
    "import binascii\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_import():\n",
    "    # import FR data (adjustments for delimeters and encoding - latin)\n",
    "    df = pd.read_csv(\"~/Dropbox/Botva/TUM/Master_Thesis/object-identification/datasets/processed_files/france_rna_processed.csv\", encoding='latin-1', sep = ';', error_bad_lines=False) \n",
    "#    df = pd.read_csv(\"~/Dropbox/Botva/TUM/Master_Thesis/object-identification/datasets/raw_files/rna_waldec_20201201_dpt_01.csv\", error_bad_lines=False)\n",
    "#    df = df.astype(str)\n",
    "    print(df)\n",
    "    print(df.dtypes)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_prepare(df):\n",
    "    df = df.apply(lambda x: x.astype(str).str.upper())\n",
    "    df['name'] = df['name'].apply(lambda x: x.replace('.',''))\n",
    "    df['name'] = df['name'].str.replace('[^0-9a-zA-Z]+', ' ')\n",
    "    df['name'] = df['name'].str.replace(' +', ' ')\n",
    "    df['name_split'] = df['name'].str.split(' ')\n",
    "    print(df)\n",
    "    print(df.dtypes)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequent_words(df_processed):\n",
    "    b = TextBlob(\"bonjour\")\n",
    "    b.detect_language()\n",
    "    print(df_processed['name'])\n",
    "    all_words = df_processed['name']    \n",
    "    all_words_cleaned = []\n",
    "\n",
    "    for text in all_words:\n",
    "        text = [x.strip(string.punctuation) for x in text]\n",
    "        all_words_cleaned.append(text)\n",
    "\n",
    "    all_words_cleaned[0]\n",
    "\n",
    "    text_words = [\" \".join(text) for text in all_words_cleaned]\n",
    "    final_text_words = \" \".join(text_words)\n",
    "    #final_text_words[:1000]\n",
    "\n",
    "    print(all_words)\n",
    "    stopwords = set(STOPWORDS)\n",
    "    stopwords.update([\"LE\",\"DE\",\"LA\",\"ET\",\"DES\",\"DU\",\"LES\",\"EN\",\"ET\",\"A\",\"POUR\",\"SUR\",\"SOU\",\"S\",\"D\",\"L\"])\n",
    "\n",
    "    wordcloud_names = WordCloud(stopwords=stopwords, background_color=\"white\", max_font_size=50, max_words=100).generate(final_text_words)\n",
    "\n",
    "    # Lines 4 to 7\n",
    "    plt.figure(figsize = (15,15))\n",
    "    plt.imshow(wordcloud_names, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    filtered_words = [word for word in final_text_words.split() if word not in stopwords]\n",
    "    counted_words = collections.Counter(filtered_words)\n",
    "\n",
    "    word_count = {}\n",
    "\n",
    "    for letter, count in counted_words.most_common(100):\n",
    "        word_count[letter] = count\n",
    "\n",
    "    for i,j in word_count.items():\n",
    "        print('Word: {0}, count: {1}'.format(i,j))\n",
    "    \n",
    "    return word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_import()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed = df_prepare(df.head(10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n = 3\n",
    "shingles_list = []\n",
    "\n",
    "texts = df_processed['name']\n",
    "hashes_list = np.array([0] * len(df_processed['name']))\n",
    "hashes_array = hashes_list\n",
    "\n",
    "text_num = 0\n",
    "shingle_num = 0\n",
    "\n",
    "for text in texts:\n",
    "#    print(text)\n",
    "#    text = text.encode()\n",
    "    shingles = [text[i:i + n] for i in range(len(text) - n + 1)]\n",
    "#    print(shingles)\n",
    "    for shingle in shingles:\n",
    "        if shingle not in shingles_list:\n",
    "            hashes_array = np.vstack((hashes_array, hashes_list)) #needs to be removed after last iteration\n",
    "            shingles_list.append(shingle)\n",
    "            hashes_array[shingle_num,text_num] = 1\n",
    "            shingle_num += 1\n",
    "        if shingle in shingles_list:\n",
    "            hashes_array[shingles_list.index(shingle),text_num] = 1\n",
    "    text_num += 1\n",
    "    \n",
    "hashes_array\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shingles_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first, set of shingles\n",
    "then, array (check from set)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.48275862, 0.3       , ..., 0.29787234, 0.17391304,\n",
       "        0.38095238],\n",
       "       [0.48275862, 1.        , 0.27586207, ..., 0.39285714, 0.1875    ,\n",
       "        0.33333333],\n",
       "       [0.3       , 0.27586207, 1.        , ..., 0.29787234, 0.43478261,\n",
       "        0.19047619],\n",
       "       ...,\n",
       "       [0.29787234, 0.39285714, 0.29787234, ..., 1.        , 0.2       ,\n",
       "        0.29166667],\n",
       "       [0.17391304, 0.1875    , 0.43478261, ..., 0.2       , 1.        ,\n",
       "        0.16666667],\n",
       "       [0.38095238, 0.33333333, 0.19047619, ..., 0.29166667, 0.16666667,\n",
       "        1.        ]])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_1 = df_processed['name']\n",
    "texts_2 = df_processed['name']\n",
    "lvn_array = np.zeros((len(texts_1),len(texts_2)))\n",
    "print(lvn_array)\n",
    "\n",
    "text_1_num = 0\n",
    "text_2_num = 0\n",
    "\n",
    "for text_1 in texts_1:\n",
    "    for text_2 in texts_2:\n",
    "        lvn_array[text_1_num,text_2_num] = Levenshtein.ratio(text_1,text_2)\n",
    "        text_2_num += 1 \n",
    "    text_2_num = 0    \n",
    "    text_1_num += 1 \n",
    "lvn_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Levenshtein.ratio(df_processed['name'],df_processed['name'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textdistance import levenshtein\n",
    "\n",
    "df_1 = df_processed['name']\n",
    "df_2 = df_processed['name']\n",
    "df_1.apply(lambda x: levenshtein.distance(df_1['name'], df_2['name']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install textdistance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
