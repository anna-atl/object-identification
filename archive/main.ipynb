{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         local_id                                               name country  \\\n",
      "0      W011000001                                         VENT D'EST  FRANCE   \n",
      "1      W011000002                                UNE NOTE DE PARTAGE  FRANCE   \n",
      "2      W011000003                                         CHANDELAIN  FRANCE   \n",
      "3      W011000004                                   LEYMENT-MATERIEL  FRANCE   \n",
      "4      W011000005                          ASSOCIATION 'TOILES EMOI'  FRANCE   \n",
      "...           ...                                                ...     ...   \n",
      "18119  W014004922                                  AU BONHEUR D'ELDA  FRANCE   \n",
      "18120  W014004923                                 JE S'AIME ET VOUS?  FRANCE   \n",
      "18121  W014004924  'ASSOCIATION COMMUNALE DE CHASSE AGREE DE MART...  FRANCE   \n",
      "18122  W014004925  'ASSOCIATION COMMUNALE DE CHASSE AGREEE DE GRO...  FRANCE   \n",
      "18123  W014004926                                    L'ATTITUDE NORD  FRANCE   \n",
      "\n",
      "       state               city    zip                     street  \\\n",
      "0        NaN        Montpellier  34090      99  RUE SALVADOR DALI   \n",
      "1        NaN          Dieulefit  26220  1995, ROUTE DE SOUSPIERRE   \n",
      "2        NaN             Cuzieu   1300        234 RTE de l'Eglise   \n",
      "3        NaN            Leyment   1150        15 Rue de Valllière   \n",
      "4        NaN  Ambérieu-en-Bugey   1500         3 avenue de Verdun   \n",
      "...      ...                ...    ...                        ...   \n",
      "18119    NaN             Cerdon   1450        278 A PONT DE PREAU   \n",
      "18120    NaN         Valserhône   1200   218  RUE DE LA TOURNETTE   \n",
      "18121    NaN          Martignat   1100                              \n",
      "18122    NaN          Groissiat   1100                              \n",
      "18123    NaN              Giron   1130    289  rue du Grand Golet   \n",
      "\n",
      "                                       url  industry  \n",
      "0                                      NaN       NaN  \n",
      "1                                      NaN       NaN  \n",
      "2                                      NaN       NaN  \n",
      "3                                      NaN       NaN  \n",
      "4                                      NaN       NaN  \n",
      "...                                    ...       ...  \n",
      "18119                                  NaN       NaN  \n",
      "18120                                  NaN       NaN  \n",
      "18121                                  NaN       NaN  \n",
      "18122                                  NaN       NaN  \n",
      "18123  www.lattitudenordchien-traineau.com       NaN  \n",
      "\n",
      "[18124 rows x 9 columns]\n",
      "local_id     object\n",
      "name         object\n",
      "country      object\n",
      "state       float64\n",
      "city         object\n",
      "zip           int64\n",
      "street       object\n",
      "url          object\n",
      "industry    float64\n",
      "dtype: object\n",
      "         local_id                                               name country  \\\n",
      "0      W011000001                                         VENT D EST  FRANCE   \n",
      "1      W011000002                                UNE NOTE DE PARTAGE  FRANCE   \n",
      "2      W011000003                                         CHANDELAIN  FRANCE   \n",
      "3      W011000004                                   LEYMENT MATERIEL  FRANCE   \n",
      "4      W011000005                           ASSOCIATION TOILES EMOI   FRANCE   \n",
      "...           ...                                                ...     ...   \n",
      "18119  W014004922                                  AU BONHEUR D ELDA  FRANCE   \n",
      "18120  W014004923                                 JE S AIME ET VOUS   FRANCE   \n",
      "18121  W014004924   ASSOCIATION COMMUNALE DE CHASSE AGREE DE MART...  FRANCE   \n",
      "18122  W014004925   ASSOCIATION COMMUNALE DE CHASSE AGREEE DE GRO...  FRANCE   \n",
      "18123  W014004926                                    L ATTITUDE NORD  FRANCE   \n",
      "\n",
      "      state               city    zip                     street  \\\n",
      "0       NAN        MONTPELLIER  34090      99  RUE SALVADOR DALI   \n",
      "1       NAN          DIEULEFIT  26220  1995, ROUTE DE SOUSPIERRE   \n",
      "2       NAN             CUZIEU   1300        234 RTE DE L'EGLISE   \n",
      "3       NAN            LEYMENT   1150        15 RUE DE VALLLIÈRE   \n",
      "4       NAN  AMBÉRIEU-EN-BUGEY   1500         3 AVENUE DE VERDUN   \n",
      "...     ...                ...    ...                        ...   \n",
      "18119   NAN             CERDON   1450        278 A PONT DE PREAU   \n",
      "18120   NAN         VALSERHÔNE   1200   218  RUE DE LA TOURNETTE   \n",
      "18121   NAN          MARTIGNAT   1100                              \n",
      "18122   NAN          GROISSIAT   1100                              \n",
      "18123   NAN              GIRON   1130    289  RUE DU GRAND GOLET   \n",
      "\n",
      "                                       url industry  \\\n",
      "0                                      NAN      NAN   \n",
      "1                                      NAN      NAN   \n",
      "2                                      NAN      NAN   \n",
      "3                                      NAN      NAN   \n",
      "4                                      NAN      NAN   \n",
      "...                                    ...      ...   \n",
      "18119                                  NAN      NAN   \n",
      "18120                                  NAN      NAN   \n",
      "18121                                  NAN      NAN   \n",
      "18122                                  NAN      NAN   \n",
      "18123  WWW.LATTITUDENORDCHIEN-TRAINEAU.COM      NAN   \n",
      "\n",
      "                                              name_split  \n",
      "0                                         [VENT, D, EST]  \n",
      "1                               [UNE, NOTE, DE, PARTAGE]  \n",
      "2                                           [CHANDELAIN]  \n",
      "3                                    [LEYMENT, MATERIEL]  \n",
      "4                          [ASSOCIATION, TOILES, EMOI, ]  \n",
      "...                                                  ...  \n",
      "18119                             [AU, BONHEUR, D, ELDA]  \n",
      "18120                          [JE, S, AIME, ET, VOUS, ]  \n",
      "18121  [, ASSOCIATION, COMMUNALE, DE, CHASSE, AGREE, ...  \n",
      "18122  [, ASSOCIATION, COMMUNALE, DE, CHASSE, AGREEE,...  \n",
      "18123                                [L, ATTITUDE, NORD]  \n",
      "\n",
      "[18124 rows x 10 columns]\n",
      "local_id      object\n",
      "name          object\n",
      "country       object\n",
      "state         object\n",
      "city          object\n",
      "zip           object\n",
      "street        object\n",
      "url           object\n",
      "industry      object\n",
      "name_split    object\n",
      "dtype: object\n",
      "              0  doc_1  doc_2  \\\n",
      "0      1.000000      0  10173   \n",
      "1      0.617647      6   1098   \n",
      "2      1.000000     18    456   \n",
      "3      1.000000     18    469   \n",
      "4      1.000000     18    550   \n",
      "...         ...    ...    ...   \n",
      "13952  0.743590  15085  15358   \n",
      "13953  0.727273  15223  15898   \n",
      "13954  0.780488  15890  17759   \n",
      "13955  0.682540  16847  18122   \n",
      "13956  0.829787  17013  17526   \n",
      "\n",
      "                                                  name_x  \\\n",
      "0                                             VENT D EST   \n",
      "1                          COMITE D ANIMATION DE LEYMENT   \n",
      "2                                 SOU DES ECOLES LAIQUES   \n",
      "3                                 SOU DES ECOLES LAIQUES   \n",
      "4                                 SOU DES ECOLES LAIQUES   \n",
      "...                                                  ...   \n",
      "13952              AMICALE DES SAPEURS POMPIERS DE SERGY   \n",
      "13953                                         L ESCARGOT   \n",
      "13954             ASSOCIATION SPORTIVE BOULISTE DE PERON   \n",
      "13955   ASSOCIATION INTERCOMMUNALE DE CHASSE AGREEE D...   \n",
      "13956     AMICALE DE LA CLASSE 90 DU BASSIN BELLEGARDIEN   \n",
      "\n",
      "                                                  name_y  \n",
      "0                                             VENT D EST  \n",
      "1                         COMITE D ANIMATION DE CLEYZIEU  \n",
      "2                                 SOU DES ECOLES LAIQUES  \n",
      "3                                 SOU DES ECOLES LAIQUES  \n",
      "4                                 SOU DES ECOLES LAIQUES  \n",
      "...                                                  ...  \n",
      "13952           AMICALE DES SAPEURS POMPIERS DE SAUVERNY  \n",
      "13953                                      L ESCARGOT 20  \n",
      "13954            ASSOCIATION SPORTIVE BOULISTE DE PONCIN  \n",
      "13955   ASSOCIATION COMMUNALE DE CHASSE AGREEE DE GRO...  \n",
      "13956     AMICALE DE LA CLASSE 67 DU BASSIN BELLEGARDIEN  \n",
      "\n",
      "[13957 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "%run main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load main.py\n",
    "import pandas as pd\n",
    "#from wordcloud import WordCloud, STOPWORDS\n",
    "import sys\n",
    "import numpy as np\n",
    "import string\n",
    "import collections\n",
    "from textblob import TextBlob\n",
    "import Levenshtein\n",
    "#import binascii\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "def df_import():\n",
    "    # import FR data (adjustments for delimeters and encoding - latin)\n",
    "    wordbook_name = \"~/Dropbox/Botva/TUM/Master_Thesis/datasets/processed_files/france_rna_processed.csv\"\n",
    "    df = pd.read_csv(wordbook_name, encoding='latin-1', sep = ';', error_bad_lines=False)\n",
    "#    df = pd.read_csv(\"~/Dropbox/Botva/TUM/Master_Thesis/object-identification/datasets/raw_files/rna_waldec_20201201_dpt_01.csv\", error_bad_lines=False)\n",
    "#    df = df.astype(str)\n",
    "    print(df)\n",
    "    print(df.dtypes)\n",
    "    return df\n",
    "\n",
    "def df_prepare(df):\n",
    "    df = df.apply(lambda x: x.astype(str).str.upper())\n",
    "    df['name'] = df['name'].apply(lambda x: x.replace('.',''))\n",
    "    df['name'] = df['name'].str.replace('[^0-9a-zA-Z]+', ' ')\n",
    "    df['name'] = df['name'].str.replace(' +', ' ')\n",
    "    df['name_split'] = df['name'].str.split(' ')\n",
    "    print(df)\n",
    "    print(df.dtypes)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def frequent_words(df_processed):\n",
    "    b = TextBlob(\"bonjour\")\n",
    "    b.detect_language()\n",
    "    print(df_processed['name'])\n",
    "    all_words = df_processed['name']\n",
    "    all_words_cleaned = []\n",
    "\n",
    "    for text in all_words:\n",
    "        text = [x.strip(string.punctuation) for x in text]\n",
    "        all_words_cleaned.append(text)\n",
    "\n",
    "    text_words = [\" \".join(text) for text in all_words_cleaned]\n",
    "    final_text_words = \" \".join(text_words)\n",
    "    # final_text_words[:1000]\n",
    "\n",
    "    print(all_words)\n",
    "    stopwords = set(STOPWORDS)\n",
    "    stopwords.update([\"LE\", \"DE\", \"LA\", \"ET\", \"DES\", \"DU\", \"LES\", \"EN\", \"ET\", \"A\", \"POUR\", \"SUR\", \"SOU\", \"S\", \"D\", \"L\"])\n",
    "\n",
    "    wordcloud_names = WordCloud(stopwords=stopwords, background_color=\"white\", max_font_size=50,\n",
    "                                max_words=100).generate(final_text_words)\n",
    "\n",
    "    # Lines 4 to 7\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    plt.imshow(wordcloud_names, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    filtered_words = [word for word in final_text_words.split() if word not in stopwords]\n",
    "    counted_words = collections.Counter(filtered_words)\n",
    "\n",
    "    word_count = {}\n",
    "\n",
    "    for letter, count in counted_words.most_common(100):\n",
    "        word_count[letter] = count\n",
    "\n",
    "    for i, j in word_count.items():\n",
    "        print('Word: {0}, count: {1}'.format(i, j))\n",
    "\n",
    "    return word_count\n",
    "\n",
    "def levenstein_distance(texts_1,texts_2):\n",
    "    lvn_array = np.zeros((len(texts_1),len(texts_2)))\n",
    "    print(lvn_array)\n",
    "\n",
    "    text_1_num = 0\n",
    "    text_2_num = 0\n",
    "\n",
    "    for text_1 in texts_1:\n",
    "        for text_2 in texts_2:\n",
    "            lvn_array[text_1_num,text_2_num] = Levenshtein.ratio(text_1,text_2)\n",
    "            text_2_num += 1\n",
    "        text_2_num = 0\n",
    "        text_1_num += 1\n",
    "    return lvn_array\n",
    "\n",
    "#creating shingles_dict\n",
    "def create_shingles_dict(texts,k):\n",
    "    shingles_list = set()\n",
    "\n",
    "    for text in texts:\n",
    "    #    print(text)\n",
    "    #    text = text.encode()\n",
    "        if len(text) >= k:\n",
    "            shingles = [text[i:i + k] for i in range(len(text) - k + 1)]\n",
    "        else:\n",
    "            shingles = [text + ' ' * (k - len(text))]\n",
    "    #    print(shingles)\n",
    "        for shingle in shingles:\n",
    "            if shingle not in shingles_list: #check, maybe if is not needed bc its a set\n",
    "                shingles_list.add(shingle)\n",
    "\n",
    "    shingles_list = sorted(shingles_list)\n",
    "    shingles_dict = dict(zip(shingles_list,range(len(shingles_list))))\n",
    "    return shingles_list, shingles_dict\n",
    "\n",
    "#converting docs to shingles\n",
    "def create_doc_shingles(texts,k):\n",
    "    docs = [[] for i in range(len(texts))]\n",
    "\n",
    "    for doc, text in zip(docs, texts):\n",
    "        if len(text) >= k:\n",
    "            shingles = [text[i:i + k] for i in range(len(text) - k + 1)]\n",
    "        else:\n",
    "            shingles = [text + ' ' * (k - len(text))]\n",
    "        for shingle in shingles:\n",
    "            doc.append(shingles_dict[shingle])\n",
    "    return docs\n",
    "\n",
    "#creating signatures array\n",
    "def create_signatures_array(docs,shingles_list,signature_size):\n",
    "    signatures = np.zeros((signature_size, len(docs))) #create an array\n",
    "\n",
    "    shingles_shuffled = [i for i in range(len(shingles_list))]\n",
    "\n",
    "    for signature in signatures:\n",
    "        random.shuffle(shingles_shuffled)\n",
    "        for doc_index, doc in enumerate(docs):\n",
    "            doc_a = [shingles_shuffled[i] for i in doc]\n",
    "            signature[doc_index] = min(doc_a)\n",
    "    return signatures\n",
    "\n",
    "def create_buckets(signatures,bands_number):\n",
    "    r = int(len(signatures)/bands_number) #rows per band\n",
    "\n",
    "    buckets_bands = [{} for i in range(bands_number)]\n",
    "\n",
    "    for band, buckets in zip(range(bands_number),buckets_bands):\n",
    "        for i in range(len(signatures[0])):\n",
    "            buckets.setdefault(tuple(signatures[band*r:band*r+r,i]),[]).append(i)\n",
    "        filtered = {key:item for key, item in buckets.items() if len(item)>1}\n",
    "        buckets.clear()\n",
    "        buckets.update(filtered)\n",
    "    return buckets_bands\n",
    "\n",
    "def jaccard(list1, list2):\n",
    "    intersection = len(set(list1).intersection(list2))\n",
    "    union = (len(set(list1)) + len(set(list2))) - intersection\n",
    "    return intersection/union\n",
    "\n",
    "def create_matches(buckets_bands):\n",
    "    matches = {}\n",
    "    for buckets in buckets_bands:\n",
    "        for key, values_list in buckets.items():\n",
    "            for value_1 in values_list:\n",
    "                for value_2 in values_list:\n",
    "                    if value_2 > value_1 and (value_1,value_2) not in matches:\n",
    "                        matches.setdefault((value_1,value_2),[]).append(jaccard(docs[value_1], docs[value_2]))\n",
    "                        #print(value_1)\n",
    "                        #print(value_2)\n",
    "    #                    print(texts[value_1])\n",
    "    #                    print(texts[value_2])\n",
    "    #                    print(jaccard(docs[value_1], docs[value_2]))\n",
    "    return matches\n",
    "\n",
    "#generate df with all potential matches\n",
    "def create_df_with_attributes(matches,texts):\n",
    "    df_matches = pd.DataFrame.from_dict(matches, orient='index')\n",
    "    df_matches['matches_tuple'] = df_matches.index\n",
    "    df_matches[['doc_1','doc_2']] = pd.DataFrame(df_matches['matches_tuple'].tolist(), index=df_matches.index)\n",
    "    df_matches = df_matches.drop(['matches_tuple'], axis=1)\n",
    "    df_matches = df_matches.reset_index(drop=True)\n",
    "\n",
    "    df_texts = pd.DataFrame(texts)\n",
    "    df_texts['index'] = df_texts.index\n",
    "\n",
    "    df_matches_full = pd.merge(df_matches, df_texts,  how='left', left_on=['doc_1'], right_on = ['index'])\n",
    "    df_matches_full = df_matches_full.drop(['index'], axis=1)\n",
    "    df_matches_full = pd.merge(df_matches_full, df_texts,  how='left', left_on=['doc_2'], right_on = ['index'])\n",
    "    df_matches_full = df_matches_full.drop(['index'], axis=1)\n",
    "    return df_matches_full\n",
    "\n",
    "df = df_import()\n",
    "df_processed = df_prepare(df)\n",
    "\n",
    "k = 3 #shingles size\n",
    "texts = df_processed['name']\n",
    "shingles_list, shingles_dict = create_shingles_dict(texts,k)\n",
    "\n",
    "docs = create_doc_shingles(texts,k)\n",
    "\n",
    "signature_size = 50\n",
    "signatures = create_signatures_array(docs,shingles_list,signature_size)\n",
    "\n",
    "bands_number = 5\n",
    "buckets_bands = create_buckets(signatures,bands_number)\n",
    "\n",
    "#threshold = 0.7\n",
    "matches = create_matches(buckets_bands)\n",
    "\n",
    "df_matches_full = create_df_with_attributes(matches,texts)\n",
    "print(df_matches_full)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:No traceback has been produced, nothing to debug.\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
